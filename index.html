<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="MIDAS: Multimodal Interactive Digital-humAn Synthesis via Real-time Autoregressive Video Generation">
  <meta name="keywords" content="AR">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MIDAS: Multimodal Interactive Digital-humAn Synthesis via Real-time Autoregressive Video Generation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/robot.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <!-- <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>
  </div> -->
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">MIDAS: Multimodal Interactive Digital-humAn Synthesis via Real-time Autoregressive Video Generation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://chenmingthu.github.io/chenming.github.io/">Ming Chen</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://chenmingthu.github.io/chenming.github.io/">Liyuan Cui</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://chenmingthu.github.io/chenming.github.io/">Haoxian Zhang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://chenmingthu.github.io/chenming.github.io/">Wenyuan Zhang</a><sup>1,3</sup>,</span>
            <span class="author-block">
              <a href="https://chenmingthu.github.io/chenming.github.io/">Yan Zhou</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://chenmingthu.github.io/chenming.github.io/">Xiaohan Li</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://chenmingthu.github.io/chenming.github.io/">Xiaoqiang Liu</a><sup>1</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Kuaishou Technology</span>
            <span class="author-block"><sup>2</sup>Zhejiang University</span>
            <span class="author-block"><sup>2</sup>Tsinghua University</span>
          </div>

          
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="dialogue" autoplay controls muted loop playsinline height="100%">
        <source src="./static/videos/dialogue.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Integrate with multimodal large language models such as GPT-4o to enable conversational digital human systems.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Interactive digital human video generation has attracted widespread research attention and has achieved remarkable progress. 
            However, most existing portrait video generation approaches rely on pretrained video models as renderers, 
            which significantly increases generation latency and computational cost. 
            In this work, we anchor our study in the interactive digital human scenario and propose an autoregressive real-time video generation framework. 
            Our system enables real-time portrait video synthesis with multimodal control, 
            achieved through minimal modifications to a standard large language model (LLM). 
            By incorporating a Diffusion Loss and adopting a frame-level autoregressive strategy, 
            the proposed method features low latency, high efficiency, and fine-grained multimodal controllability.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-4">Autoregressive Generation Model</h2>
    <div class="columns is-centered has-text-centered">
      <img src="./static/images/inference.png"
        class="interpolation-image"
        alt="Interpolate start reference image."/>
    </div>
    <h2 class="subtitle has-text-centered">
      Our streaming generation approach processes
      inputs in chunks.
    </h2>
    <p>
      To enable efficient streaming generation, we organize inputs and outputs into logical chunks, where
      each chunk contains a concatenated sequence of audio tokens, pose tokens, and frame tokens.
      This structured token organization facilitates both streaming control input and sequential output
      generation, allowing for real-time responsiveness while maintaining contextual coherence across chunks.
    </p>
    <p>
      We design a specialized frame-level causal attention mask to optimize for both streaming generation
      and output quality. This mask permits each token to attend only to tokens from previous frames and
      to all tokens within its own frame. This hybrid approach—causal attention between frames and full
      attention within frames—balances temporal consistency with spatial coherence, critically important
      for high-quality visual outputs.
    </p>
    <p>
      For efficient inference, we implement the diffusion head as an MLP and utilize flow matching to achieve high sampling efficiency. 
      At inference time, the diffusion head completes the generation process within 5–10 sampling iterations, enabling real-time performance.
    </p>
  </div>
</section>

<section>
  <div class="container is-max-desktop">
    <h2 class="title is-4">Highly Compressed Video Tokenizer</h2>
    <div class="columns is-centered has-text-centered">
      <img src="./static/images/vae.png"
        class="interpolation-image"
        alt="Interpolate start reference image."/>
    </div>
    <h2 class="subtitle has-text-centered">
      Illustration of streaming autoencoding.
    </h2>
    <p>
      For real-time AR video generation, our streaming autoencoder meets two key criteria: 
      (1) maintain high reconstruction fidelity under strong spatial compression (64 in our implementation) to enable efficient processing by LLM, 
      and (2) preserve and explicitly model the temporal dimension during encoding and decoding to support autoregressive generation with coherent, 
      flicker-free outputs. 
    </p>
    <p>
      Training is performed in three stages: 
      (i) pretraining of the Deep Compression Autoencoder (DC-AE),
      (ii) temporal module training, where causal 3D convolutions and RoPE-based attention layers are inserted after each spatial layer, 
      and (iii) joint fine-tuning. 
    </p>
    <p>
      During inference, streaming encoding and decoding are performed frame by frame with cached temporal features (feature maps and key/value caches), 
      leveraging a short history buffer to ensure real-time autoregressive generation with temporal consistency.
    </p>
  </div>
</section>

<section>
  <div class="container is-max-desktop">
    <h2 class="title is-4">Dataset Construction and Processing</h2>
    <div class="columns is-centered has-text-centered">
      <img src="./static/images/dataproc.png"
        class="interpolation-image"
        alt="Interpolate start reference image."/>
    </div>
    <h2 class="subtitle has-text-centered">
      Illustration of data collection and processing pipeline.
    </h2>
    <p>
      The training dataset combines single-person and two-person speech content from three sources: 
      (1) Publicly available benchmarks (VoxCeleb1/2, TED-LRS); 
      (2) curated online videos including podcasts, interviews, talk shows, and speeches; and 
      (3) custom-recorded sessions featuring controlled two-person interactions.
    </p>
    <p>
      Data processing consists of three stages: preprocessing, annotation and synthetic data construction, and post-processing. 
      Preprocessing: Temporal segmentation is performed using shot boundary detection and active speaker detection (ASD), followed by filtering of human subjects via face and body detection. 
      Each segmented clip then undergoes rigorous evaluation for visual quality, audio quality, and lip synchronization. 
      Annotation and Data Construction: This stage includes quality assessment, caption, emotion label, and automatic speech recognition (ASR) transcription. 
      A part of single-person data is adapted into conversational formats using semantic analysis and text-to-speech (TTS) synthesis. 
      Post-processing: Annotated data undergoes manual review combined with automatic sampling to ensure balanced, high-quality subsets. 
    </p>
    <p>
      The final dataset contains approximately 20,000 hours of pre-training video data and over 400 hours of duplex fine-tuning (SFT) data.
    </p>
  </div>
</section>

<!-- <h2 class="subtitle has-text-centered">
  With fine-tuning on a designated character representation, the system can support multilingual audio-driven generation of long-duration videos.
</h2> -->
<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline style="width:100%;height:auto">
            <source src="./static/videos/eng_long.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline style="width:100%;height:auto">
            <source src="./static/videos/jap_long.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline style="width:100%;height:auto">
            <source src="./static/videos/chn_long.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Long video results</h2>
    <p>
      With fine-tuning on a designated character representation, the system can support multilingual audio-driven generation of long-duration videos.
    </p>
    <div class="columns is-centered">

      <div class="column">
        <div class="content">
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/eng_long.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>

      <div class="column">
        <div class="columns is-centered">
          <div class="column content">
            <video id="matting-video" autoplay controls muted playsinline height="100%">
              <source src="./static/videos/jap_long.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
      <div class="column">
        <div class="columns is-centered">
          <div class="column content">
            <video id="matting-video" autoplay controls muted playsinline height="100%">
              <source src="./static/videos/chn_long.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>

    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <!-- Interpolating. -->
        <h3 class="title is-4">Duplex Data Fine-Tuning</h3>
        <div class="content has-text-justified">
          <p>
            The pretrained model is further adapted on 400 hours of full-duplex conversational data, 
            allowing it to condition on dual-stream audio inputs and produce videos 
            with seamless transitions between talking and listening modes.
          </p>
        </div>
        <div class="columns is-centered">

          <div class="column">
            <div class="content">
              <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/duplex_1.mp4"
                        type="video/mp4">
              </video>
            </div>
          </div>
    
          <div class="column">
            <div class="columns is-centered">
              <div class="column content">
                <video id="matting-video" autoplay controls muted playsinline height="100%">
                  <source src="./static/videos/duplex_2.mp4"
                          type="video/mp4">
                </video>
              </div>
    
            </div>
          </div>
          <div class="column">
            <div class="columns is-centered">
              <div class="column content">
                <video id="matting-video" autoplay controls muted playsinline height="100%">
                  <source src="./static/videos/duplex_3.mp4"
                          type="video/mp4">
                </video>
              </div>
    
            </div>
          </div>
          <div class="column">
            <div class="columns is-centered">
              <div class="column content">
                <video id="matting-video" autoplay controls muted playsinline height="100%">
                  <source src="./static/videos/duplex_4.mp4"
                          type="video/mp4">
                </video>
              </div>
    
            </div>
          </div>
          <div class="column">
            <div class="columns is-centered">
              <div class="column content">
                <video id="matting-video" autoplay controls muted playsinline height="100%">
                  <source src="./static/videos/duplex_5.mp4"
                          type="video/mp4">
                </video>
              </div>
    
            </div>
          </div>
        </div>

        <!-- <h3 class="title is-4">Re-rendering the input video</h3> -->
        <div class="content has-text-justified">
          <p>
            Our system enables natural turn-taking dialogue between
            digital avatars with synchronized audio-visual responses. Each avatar maintains appropriate listen-
            ing expressions when the other is speaking, and becomes animated with synchronized lip move-
            ments and facial expressions when driven by their corresponding audio input. The audio waveforms
            (visualized in blue and green) clearly indicate the speaking turns. This demonstrates the model’s
            ability to generate contextually appropriate reactions and maintain speaker identity while handling
            the complex dynamics of conversational interaction.
          </p>
        </div>

        <div class="columns is-centered">
          <div class="column">
            <div class="content">
              <img src="./static/images/dialog.png"
                class="interpolation-image"
                alt="Interpolate start reference image."/>
            </div>
          </div>

          <div class="column">
            <div class="columns is-centered">
              <div class="column content">
                <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
                  <source src="./static/videos/dia_duplex.mp4"
                          type="video/mp4">
                </video>
              </div>
            </div>
          </div>
        </div>

        <!-- <div class="content has-text-centered">
          <img src="./static/images/dialog.png"
          class="interpolation-image"
          alt="Interpolate start reference image."/>
        </div>

        <div class="content has-text-centered">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="50%">
            <source src="./static/videos/dia_duplex.mp4"
                    type="video/mp4">
          </video>
        </div> -->

      </div>
    </div>
    <!--/ Animation. -->

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Limitations</h2>
    <p>
      The current model exhibits limitations in generalization capability; 
      when using arbitrary images as the initial frame, 
      the generated videos suffer from issues in identity preservation, 
      temporal consistency, and stability, making it infeasible to perform long-duration inference while maintaining high quality. 
      Addressing these shortcomings may require larger-scale datasets and more powerful models in the future.
    </p>

    <div id="results-carousel" class="carousel results-carousel">
      <div class="item item-steve">
        <video poster="" id="steve" autoplay controls muted loop playsinline height="50%">
          <source src="./static/videos/s1.mp4"
                  type="video/mp4">
        </video>
      </div>
      <div class="item item-chair-tp">
        <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="50%">
          <source src="./static/videos/s2.mp4"
                  type="video/mp4">
        </video>
      </div>
      <div class="item item-shiba">
        <video poster="" id="shiba" autoplay controls muted loop playsinline height="50%">
          <source src="./static/videos/s3.mp4"
                  type="video/mp4">
        </video>
      </div>
      <div class="item item-fullbody">
        <video poster="" id="fullbody" autoplay controls muted loop playsinline height="50%">
          <source src="./static/videos/s4.mp4"
                  type="video/mp4">
        </video>
      </div>
      <div class="item item-blueshirt">
        <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="50%">
          <source src="./static/videos/s5.mp4"
                  type="video/mp4">
        </video>
      </div>
      <div class="item item-mask">
        <video poster="" id="mask" autoplay controls muted loop playsinline height="50%">
          <source src="./static/videos/s6.mp4"
                  type="video/mp4">
        </video>
      </div>
    </div>
  </div>
</section>


<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website source code based on the <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> project page. 
            If you want to reuse their source code, please credit them appropriately.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
